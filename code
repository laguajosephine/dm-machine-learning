#Question 1
#On sépare le jeu de données

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y)


#Question 2
#On entraine les SVM

from sklearn.svm import LinearSVC
svm = LinearSVC(C=1).fit(X_train,y_train)
print('Score: ',svm.score(X_test,y_test))

#On fait une validation croisée sur l'échantillon d'apprentissage

from sklearn.model_selection import validation_curve
C_range = np.logspace(-2,10,20)
train_scores, valid_scores = validation_curve(LinearSVC(), X_train, y_train, param_name="C", param_range=C_range, cv=5)
train_scores_mean = np.mean(train_scores,axis=1)
valid_scores_mean = np.mean(valid_scores,axis=1)

import matplotlib.pyplot as plt
plt.plot(np.log10(C_range),train_scores_mean,label="scores d'apprentissage")
plt.plot(np.log10(C_range),valid_scores_mean,label="scores de validation")
plt.legend()
plt.xlabel('log(C)')
plt.ylabel('score')
plt.show()

#Le C_best n'est probablement pas bon, à revoir
C_best = C_range[np.argmax(valid_scores_mean)]
print(C_best)

from sklearn.svm import LinearSVC
svm = LinearSVC(C=C_best).fit(X_train,y_train)
print('Score: ',svm.score(X_test,y_test))

#On défini un array y_test_predict

y_test_predict=svm.predict(X_test)


#Question 3
#On construit la matrice de confusion

from sklearn.metrics import confusion_matrix
confusion_matrix_test = confusion_matrix(y_test,y_test_predict)
print(confusion_matrix_test)

#On calcule la précision et le rappel avec la matrice de confusion
rappel = confusion_matrix_test[0][0]/(confusion_matrix_test[0][0]+confusion_matrix_test[0][1])
precision = confusion_matrix_test[0][0]/(confusion_matrix_test[0][0]+confusion_matrix_test[1][0])
print('Rappel : ',rappel, '; Precision : ',precision)

#On vérifie que nos calculs étaient bons
from sklearn.metrics import recall_score, precision_score
print('Rappel : ',recall_score(y_test,y_test_predict,pos_label=0))
print('Precision : ',precision_score(y_test,y_test_predict,pos_label=0))


#Question 4
def false_positive_rate(y_true,y_predict,pos_label):
    confusion_mat = confusion_matrix(y_true,y_predict)
    if pos_label == 0:
        fpr = confusion_mat[0][1]/(confusion_mat[0][1]+confusion_mat[1][1])
    if pos_label == 1:
        fpr = confusion_mat[1][1]/(confusion_mat[1][1]+confusion_mat[0][1])
    return fpr

print('Taux de faux positifs : ', false_positive_rate(y_test,y_test_predict,0))


#Question 5
def modified_predictor(entrees, tau):
    predictions = []
    for i in range(len(svm.decision_function(entrees))): 
        if svm.decision_function(entrees)[i] > tau:
            predictions.append(1)
        else : 
            predictions.append(0)
    return np.array(predictions)


#Question 6
#La meilleure courbe ROC possible vaudrait 0 en [0,1[ et 1 quand rappel=1. Donc une bonne famille de prédicteurs est une dont la courbe ROC est croissante convexe avec une asymptote en rappel=1. Au point rappel=0,95 on aurait un FPR faible.
#Une mauvaise courbe ROC est, à l'inverse, concave avec une asymptote en FPR=1.


#Question 7

#Valeur minimale de tau
print(min(svm.decision_function(X_train)))

#Valeur maximale de tau
print(max(svm.decision_function(X_train)))

#Considérons une liste de 100 valeurs à essayer
from random import *
valeurs = []
for i in range (0,100):
    valeurs.append(uniform(min(svm.decision_function(X_train)),max(svm.decision_function(X_train))))
valeurs.sort()

#Calculer rappel et FPR
rappel = []
fpr = []
for i in range(100):
    predictions_y_train = modified_predictor(X_train, valeurs[i])
    rappel.append(recall_score(y_train, predictions_y_train, 0))
    fpr.append(false_positive_rate(y_train, predictions_y_train, 0))

#Tracer la courbe ROC
def plot_roc_curve(rappel, FPR):
    plt.plot(rappel, FPR, label='ROC')
    plt.xlabel('Rappel')
    plt.ylabel('False Positive Rate')
    plt.legend()
    plt.show()
    
plot_roc_curve(rappel, fpr)


#Question 8
tau_fpr = []
i = 0
while i < 100:
    if rappel[i] >= 0.95 : #rappel supérieur à 0.95
        tau_fpr += [valeurs[i],fpr[i]]
    i += 1

min_fpr = 1
tau_final = 0
for i in range(len(tau_fpr)-1):
    if tau_fpr[i][1] < min_fpr:
        min_fpr = tau_fpr[i][1] #fpr le plus faible
        tau_final = tau_fpr[i][0]

#Calculer le score, rappel, précision, fpr du prédicteur sur échantillon de test
from sklearn import metrics #c'est pour le score

predicteur_final = modified_predictor(X_test, tau_final)
score_final = metrics.accuracy_score(X_test, predicteur_final)
rappel_final = recall_score(X_test, predicteur_final, pos_label = 0)
precision_finale = precision_score(X_test, predicteur_final, pos_label = 0)
fpr_final = false_positive_rate(y_test, predicteur_final, pos_label = 0)

