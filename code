#Question 1
#On sépare le jeu de données

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y)


#Question 2
#On entraine les SVM

from sklearn.svm import LinearSVC
svm = LinearSVC(C=1).fit(X_train,y_train)
print('Score: ',svm.score(X_test,y_test))

#On fait une validation croisée sur l'échantillon d'apprentissage

from sklearn.model_selection import validation_curve
C_range = np.logspace(-15,15,50)
train_scores, valid_scores = validation_curve(LinearSVC(), X_train, y_train, param_name="C", param_range=C_range, cv=5)
train_scores_mean = np.mean(train_scores,axis=1)
valid_scores_mean = np.mean(valid_scores,axis=1)

import matplotlib.pyplot as plt
plt.plot(np.log10(C_range),train_scores_mean,label="scores d'apprentissage")
plt.plot(np.log10(C_range),valid_scores_mean,label="scores de validation")
plt.legend()
plt.xlabel('log(C)')
plt.ylabel('score')
plt.show()

#Le C_best n'est probablement pas bon, à revoir
C_best = C_range[np.argmax(valid_scores_mean)]
print(C_best)

from sklearn.svm import LinearSVC
svm = LinearSVC(C=C_best).fit(X_train,y_train)
print('Score: ',svm.score(X_test,y_test))

#On défini un array y_test_predict

y_test_predict=svm.predict(X_test)


#Question 3
#On construit la matrice de confusion

from sklearn.metrics import confusion_matrix
confusion_matrix_test = confusion_matrix(y_test,y_test_predict)
print(confusion_matrix_test)

#On calcule la précision et le rappel avec la matrice de confusion
rappel = confusion_matrix_test[0][0]/(confusion_matrix_test[0][0]+confusion_matrix_test[0][1])
precision = confusion_matrix_test[0][0]/(confusion_matrix_test[0][0]+confusion_matrix_test[1][0])
print('Rappel : ',rappel, '; Precision : ',precision)

#On vérifie que nos calculs étaient bons
from sklearn.metrics import recall_score, precision_score
print('Rappel : ',recall_score(y_test,y_test_predict,pos_label=0))
print('Precision : ',precision_score(y_test,y_test_predict,pos_label=0))


#Question 4
#Ici je sais pas comment intégrer pos_label dans les lignes de code
def false_positive_rate(y_true,y_predict,pos_label):
    confusion_mat = confusion_matrix(y_true,y_predict)
    fpr = confusion_mat[0][1]/(confusion_mat[0][1]+confusion_mat[1][1])
    return fpr

print('Taux de faux positifs : ', false_positive_rate(y_test,y_test_predict,0))

#Question 5
def modified_predictor(entrees, tau):
    predictions = []
    for k in range(len(entrees)):
        predictions += [0]
    for i in range(len(entrees)-1): 
        if svm.decision_function(entrees)[i] > tau:
            predictions[i]= 1
    return np.array(predictions)

#Question 6
#La meilleure courbe ROC possible vaudrait 0 en [0,1[ et 1 quand rappel=1. Donc une bonne famille de prédicteurs est une dont la courbe ROC est croissante convexe avec une asymptote en rappel=1. Au point rappel=0,95 on aurait un FPR faible.
#Une mauvaise courbe ROC est, à l'inverse, concave avec une asymptote en FPR=1.
